{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "principal_path = '../'\n",
    "if principal_path not in sys.path:\n",
    "    sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from models.PlasticityMNISTModel import PlasticityMNISTModel\n",
    "from models.MNISTModel import MNISTModel\n",
    "from utils import load_data_filtered, find_low_activation_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 2024\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTModel(\n",
      "  (train_acc): MulticlassAccuracy()\n",
      "  (val_acc): MulticlassAccuracy()\n",
      "  (test_acc): MulticlassAccuracy()\n",
      "  (precision): MulticlassPrecision()\n",
      "  (recall): MulticlassRecall()\n",
      "  (f1): MulticlassF1Score()\n",
      "  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (layer_3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "checkpoint_ffn = 'checkpoints/mnist_model.ckpt'\n",
    "model = MNISTModel.load_from_checkpoint(checkpoint_ffn)\n",
    "model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_include = [0, 1]\n",
    "BATCH_SIZE = 32\n",
    "train_loader_filtered, val_loader_filtered, test_loader_filtered = load_data_filtered(BATCH_SIZE, classes_to_include, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\code\\ExAI\\exai\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\code\\ExAI\\exai\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 67/67 [00:00<00:00, 68.62it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           f1               0.8955720067024231\n",
      "        precision           0.8991332650184631\n",
      "         recall             0.8922649025917053\n",
      "        test_acc            0.9900709390640259\n",
      "        test_loss          0.050506915897130966\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.050506915897130966,\n",
       "  'test_acc': 0.9900709390640259,\n",
       "  'precision': 0.8991332650184631,\n",
       "  'recall': 0.8922649025917053,\n",
       "  'f1': 0.8955720067024231}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_model_trainer = pl.Trainer(max_epochs=5)\n",
    "# test model\n",
    "mnist_model_trainer.test(model, test_loader_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Layer1_Neuron1</th>\n",
       "      <th>Layer1_Neuron2</th>\n",
       "      <th>Layer1_Neuron3</th>\n",
       "      <th>Layer1_Neuron4</th>\n",
       "      <th>Layer1_Neuron5</th>\n",
       "      <th>Layer1_Neuron6</th>\n",
       "      <th>Layer1_Neuron7</th>\n",
       "      <th>Layer1_Neuron8</th>\n",
       "      <th>Layer1_Neuron9</th>\n",
       "      <th>...</th>\n",
       "      <th>Layer3_Neuron1</th>\n",
       "      <th>Layer3_Neuron2</th>\n",
       "      <th>Layer3_Neuron3</th>\n",
       "      <th>Layer3_Neuron4</th>\n",
       "      <th>Layer3_Neuron5</th>\n",
       "      <th>Layer3_Neuron6</th>\n",
       "      <th>Layer3_Neuron7</th>\n",
       "      <th>Layer3_Neuron8</th>\n",
       "      <th>Layer3_Neuron9</th>\n",
       "      <th>Layer3_Neuron10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.055817</td>\n",
       "      <td>-17.074514</td>\n",
       "      <td>-12.136678</td>\n",
       "      <td>-11.195572</td>\n",
       "      <td>-25.313034</td>\n",
       "      <td>-18.126488</td>\n",
       "      <td>-34.183750</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-16.948915</td>\n",
       "      <td>-15.565269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.122375</td>\n",
       "      <td>2.949144</td>\n",
       "      <td>2.404661</td>\n",
       "      <td>1.715258</td>\n",
       "      <td>0.338522</td>\n",
       "      <td>0.128126</td>\n",
       "      <td>1.610624</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.903490</td>\n",
       "      <td>-8.095853</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-11.127125</td>\n",
       "      <td>-27.803934</td>\n",
       "      <td>-17.198702</td>\n",
       "      <td>-14.273718</td>\n",
       "      <td>-19.698458</td>\n",
       "      <td>-13.450046</td>\n",
       "      <td>-30.231627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.827291</td>\n",
       "      <td>1.748518</td>\n",
       "      <td>0.699948</td>\n",
       "      <td>0.539016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628443</td>\n",
       "      <td>0.588888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.210146</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-11.081397</td>\n",
       "      <td>-12.736809</td>\n",
       "      <td>-10.941203</td>\n",
       "      <td>-11.521659</td>\n",
       "      <td>-11.719425</td>\n",
       "      <td>-8.667583</td>\n",
       "      <td>-8.749459</td>\n",
       "      <td>-14.620137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.043494</td>\n",
       "      <td>0.438858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.829711</td>\n",
       "      <td>2.498251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>-13.427382</td>\n",
       "      <td>-9.176719</td>\n",
       "      <td>-14.582230</td>\n",
       "      <td>-15.579443</td>\n",
       "      <td>-13.185836</td>\n",
       "      <td>-8.165977</td>\n",
       "      <td>-12.736848</td>\n",
       "      <td>-13.246180</td>\n",
       "      <td>-10.434205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187802</td>\n",
       "      <td>0.092359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.307845</td>\n",
       "      <td>1.650951</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.418967</td>\n",
       "      <td>-15.270522</td>\n",
       "      <td>-12.993325</td>\n",
       "      <td>-18.622927</td>\n",
       "      <td>-0.005147</td>\n",
       "      <td>-10.958460</td>\n",
       "      <td>-15.022395</td>\n",
       "      <td>-10.377352</td>\n",
       "      <td>-9.788157</td>\n",
       "      <td>-5.293344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number  Layer1_Neuron1  Layer1_Neuron2  Layer1_Neuron3  Layer1_Neuron4  \\\n",
       "0     7.0        0.000000        0.000000        0.000000        0.399097   \n",
       "1     2.0        0.000000        0.000000        3.122375        2.949144   \n",
       "2     1.0        1.827291        1.748518        0.699948        0.539016   \n",
       "3     0.0        0.000000        0.000000        0.614674        0.000000   \n",
       "4     4.0        0.000000        0.000000        1.187802        0.092359   \n",
       "\n",
       "   Layer1_Neuron5  Layer1_Neuron6  Layer1_Neuron7  Layer1_Neuron8  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        2.404661        1.715258        0.338522        0.128126   \n",
       "2        0.000000        0.628443        0.588888        0.000000   \n",
       "3        1.043494        0.438858        0.000000        3.829711   \n",
       "4        0.000000        0.000000        0.000000        2.307845   \n",
       "\n",
       "   Layer1_Neuron9  ...  Layer3_Neuron1  Layer3_Neuron2  Layer3_Neuron3  \\\n",
       "0        0.000000  ...      -22.055817      -17.074514      -12.136678   \n",
       "1        1.610624  ...      -15.903490       -8.095853       -0.000322   \n",
       "2        0.020213  ...      -16.210146       -0.000385      -11.081397   \n",
       "3        2.498251  ...       -0.000426      -13.427382       -9.176719   \n",
       "4        1.650951  ...      -13.418967      -15.270522      -12.993325   \n",
       "\n",
       "   Layer3_Neuron4  Layer3_Neuron5  Layer3_Neuron6  Layer3_Neuron7  \\\n",
       "0      -11.195572      -25.313034      -18.126488      -34.183750   \n",
       "1      -11.127125      -27.803934      -17.198702      -14.273718   \n",
       "2      -12.736809      -10.941203      -11.521659      -11.719425   \n",
       "3      -14.582230      -15.579443      -13.185836       -8.165977   \n",
       "4      -18.622927       -0.005147      -10.958460      -15.022395   \n",
       "\n",
       "   Layer3_Neuron8  Layer3_Neuron9  Layer3_Neuron10  \n",
       "0       -0.000019      -16.948915       -15.565269  \n",
       "1      -19.698458      -13.450046       -30.231627  \n",
       "2       -8.667583       -8.749459       -14.620137  \n",
       "3      -12.736848      -13.246180       -10.434205  \n",
       "4      -10.377352       -9.788157        -5.293344  \n",
       "\n",
       "[5 rows x 395 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"activations/activations_minist_model.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_columns(layer):\n",
    "    layer_columns = ['Number']\n",
    "    layer_columns += [c for c in df.columns if layer in c]\n",
    "    return layer_columns\n",
    "\n",
    "df_layer1 = df[get_layer_columns('Layer1')]\n",
    "df_layer2 = df[get_layer_columns('Layer2')]\n",
    "#df_layer3 = df[get_layer_columns('Layer3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold layer 1: 50.0%\n",
      "High activation neurons layer: 25\n",
      "Threshold layer 2: 50.0%\n",
      "High activation neurons layer: 56\n"
     ]
    }
   ],
   "source": [
    "threshold_layer_1 = 0.5\n",
    "print(f'Threshold layer 1: {threshold_layer_1*100}%')\n",
    "\n",
    "mask_indices_layer_1 = find_low_activation_neurons(df_layer1, 0, 1, threshold=threshold_layer_1)\n",
    "print(f'High activation neurons layer: {128 - len(mask_indices_layer_1)}')\n",
    "\n",
    "threshold_layer_2 = 0.5\n",
    "print(f'Threshold layer 2: {threshold_layer_2*100}%')\n",
    "\n",
    "mask_indices_layer_2 = find_low_activation_neurons(df_layer2, 0, 1, threshold=threshold_layer_2)\n",
    "\n",
    "print(f'High activation neurons layer: {256 - len(mask_indices_layer_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlasticityMNISTModel(\n",
      "  (train_acc): MulticlassAccuracy()\n",
      "  (val_acc): MulticlassAccuracy()\n",
      "  (test_acc): MulticlassAccuracy()\n",
      "  (precision): MulticlassPrecision()\n",
      "  (recall): MulticlassRecall()\n",
      "  (f1): MulticlassF1Score()\n",
      "  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (layer_3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load plasticy model\n",
    "plasticy_model = PlasticityMNISTModel.load_from_checkpoint(checkpoint_ffn, mask_indices_layer_1=mask_indices_layer_1, mask_indices_layer_2=mask_indices_layer_2)\n",
    "print(plasticy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy  | 0     \n",
      "1 | val_acc   | MulticlassAccuracy  | 0     \n",
      "2 | test_acc  | MulticlassAccuracy  | 0     \n",
      "3 | precision | MulticlassPrecision | 0     \n",
      "4 | recall    | MulticlassRecall    | 0     \n",
      "5 | f1        | MulticlassF1Score   | 0     \n",
      "6 | layer_1   | Linear              | 100 K \n",
      "7 | layer_2   | Linear              | 33.0 K\n",
      "8 | layer_3   | Linear              | 2.6 K \n",
      "--------------------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\ExAI\\exai\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "c:\\code\\ExAI\\exai\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: |          | 0/? [00:00<?, ?it/s]Failed to save model graph: 'ExperimentWriter' object has no attribute 'add_graph'\n",
      "Epoch 0: 100%|██████████| 317/317 [00:01<00:00, 164.47it/s, v_num=8, train_loss=1.71e-6, train_acc=1.000, val_loss=9.31e-5, val_acc=1.000]Failed to save weights histogram: 'ExperimentWriter' object has no attribute 'add_histogram'\n",
      "Epoch 1: 100%|██████████| 317/317 [00:01<00:00, 166.33it/s, v_num=8, train_loss=1.43e-7, train_acc=1.000, val_loss=0.000907, val_acc=1.000]Failed to save weights histogram: 'ExperimentWriter' object has no attribute 'add_histogram'\n",
      "Epoch 2: 100%|██████████| 317/317 [00:02<00:00, 153.95it/s, v_num=8, train_loss=0.000, train_acc=1.000, val_loss=2.41e-5, val_acc=1.000]    Failed to save weights histogram: 'ExperimentWriter' object has no attribute 'add_histogram'\n",
      "Epoch 3: 100%|██████████| 317/317 [00:02<00:00, 147.95it/s, v_num=8, train_loss=1.19e-8, train_acc=1.000, val_loss=1.34e-5, val_acc=1.000] Failed to save weights histogram: 'ExperimentWriter' object has no attribute 'add_histogram'\n",
      "Epoch 4: 100%|██████████| 317/317 [00:02<00:00, 147.48it/s, v_num=8, train_loss=0.000, train_acc=1.000, val_loss=8.57e-6, val_acc=1.000]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to save weights histogram: 'ExperimentWriter' object has no attribute 'add_histogram'\n",
      "Epoch 4: 100%|██████████| 317/317 [00:02<00:00, 147.07it/s, v_num=8, train_loss=0.000, train_acc=1.000, val_loss=8.57e-6, val_acc=1.000]\n"
     ]
    }
   ],
   "source": [
    "plasticy_model_trainer = pl.Trainer(max_epochs=5)\n",
    "plasticy_model_trainer.fit(plasticy_model, train_loader_filtered, val_loader_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\code\\ExAI\\exai\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 67/67 [00:00<00:00, 143.15it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           f1               0.9995154142379761\n",
      "        precision           0.9996017813682556\n",
      "         recall             0.9994596838951111\n",
      "        test_acc            0.9995272159576416\n",
      "        test_loss          0.0035234333481639624\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.0035234333481639624,\n",
       "  'test_acc': 0.9995272159576416,\n",
       "  'precision': 0.9996017813682556,\n",
       "  'recall': 0.9994596838951111,\n",
       "  'f1': 0.9995154142379761}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plasticy_model_trainer.test(plasticy_model, test_loader_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint\n",
    "plasticy_model_trainer.save_checkpoint('checkpoints/plasticy_mnist_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### study weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlasticityMNISTModel(\n",
      "  (train_acc): MulticlassAccuracy()\n",
      "  (val_acc): MulticlassAccuracy()\n",
      "  (test_acc): MulticlassAccuracy()\n",
      "  (precision): MulticlassPrecision()\n",
      "  (recall): MulticlassRecall()\n",
      "  (f1): MulticlassF1Score()\n",
      "  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (layer_3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load plasticy model\n",
    "plasticy_model = PlasticityMNISTModel.load_from_checkpoint('checkpoints/plasticy_mnist_model.ckpt', mask_indices_layer_1=mask_indices_layer_1, mask_indices_layer_2=mask_indices_layer_2)\n",
    "plasticy_model.eval()\n",
    "print(plasticy_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0270, -0.0021,  0.0056,  ...,  0.0151,  0.0341,  0.0220],\n",
       "        [ 0.0315,  0.0085, -0.0135,  ...,  0.0088,  0.0255,  0.0351],\n",
       "        [-0.0356,  0.0065, -0.0031,  ...,  0.0211,  0.0130, -0.0242],\n",
       "        ...,\n",
       "        [ 0.0276,  0.0313,  0.0158,  ...,  0.0118,  0.0186,  0.0071],\n",
       "        [ 0.0226,  0.0201,  0.0059,  ...,  0.0301,  0.0083,  0.0342],\n",
       "        [ 0.0328, -0.0055, -0.0058,  ...,  0.0305,  0.0073,  0.0319]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plasticy_model.layer_1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTModel(\n",
      "  (train_acc): MulticlassAccuracy()\n",
      "  (val_acc): MulticlassAccuracy()\n",
      "  (test_acc): MulticlassAccuracy()\n",
      "  (precision): MulticlassPrecision()\n",
      "  (recall): MulticlassRecall()\n",
      "  (f1): MulticlassF1Score()\n",
      "  (layer_1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (layer_3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#load minist model\n",
    "mnist_model = MNISTModel.load_from_checkpoint(checkpoint_ffn)\n",
    "mnist_model.eval()\n",
    "print(mnist_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
