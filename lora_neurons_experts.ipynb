{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "    \n",
    "from typing import List\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 2024\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project_name = 'MNIST_LORA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download only\n",
    "        datasets.MNIST(root=self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(root=self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Transform and split datasets\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = datasets.MNIST(root=self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(root=self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "# Example of how to use the MNISTDataModule\n",
    "batch_size = 64\n",
    "mnist_data = MNISTDataModule(data_dir='./data', batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "random_seed = 123\n",
    "learning_rate = 0.005\n",
    "num_epochs = 2\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "class MultilayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log('test_loss', loss)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "model = MultilayerPerceptron(num_features, num_hidden_1, num_hidden_2, num_classes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "#wandb_logger = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"baseline\", group=\"baseline\", save_dir=\"lightning_logs\")\n",
    "trainer = Trainer(max_epochs=num_epochs) #, logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 136 K \n",
      "--------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:08<00:00, 106.92it/s, v_num=36, train_loss_step=0.0947, train_acc_step=0.917, val_loss_step=0.000484, val_acc_step=1.000, val_loss_epoch=0.128, val_acc_epoch=0.964, train_loss_epoch=0.115, train_acc_epoch=0.965]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:08<00:00, 106.69it/s, v_num=36, train_loss_step=0.0947, train_acc_step=0.917, val_loss_step=0.000484, val_acc_step=1.000, val_loss_epoch=0.128, val_acc_epoch=0.964, train_loss_epoch=0.115, train_acc_epoch=0.965]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 260.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9661999940872192     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11397630721330643    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9661999940872192    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11397630721330643   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.11397630721330643, 'test_acc': 0.9661999940872192}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRa methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)\n",
    "\n",
    "    \n",
    "# This DoRA code is equivalent to LinearWithDoRA\n",
    "# Code inspired by https://github.com/catid/dora/blob/main/dora.py\n",
    "class LinearWithDoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "        \n",
    "        self.m = nn.Parameter(\n",
    "            self.linear.weight.norm(p=2, dim=0, keepdim=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        numerator = self.linear.weight + self.lora.alpha*lora.T\n",
    "        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n",
    "        directional_component = numerator / denominator\n",
    "        new_weight = self.m * directional_component\n",
    "        return F.linear(x, new_weight, self.linear.bias)\n",
    "    \n",
    "class LinearWithDoRAMoE(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha, num_experts):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "        \n",
    "        # Crear múltiples m como expertos\n",
    "        self.m_experts = nn.ParameterList([\n",
    "            nn.Parameter(self.linear.weight.norm(p=2, dim=1, keepdim=True)) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Capa lineal para generar puntuaciones de enrutamiento basadas en la entrada x\n",
    "        self.routing_weights = nn.Parameter(torch.zeros(1, num_experts))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        numerator = self.linear.weight + self.lora.alpha * lora.T\n",
    "        denominator = numerator.norm(p=2, dim=1, keepdim=True)\n",
    "        directional_component = numerator / denominator\n",
    "\n",
    "        # Generar puntuaciones de enrutamiento para cada experto usando la entrada x\n",
    "        routing_probs = F.softmax(self.routing_weights, dim=-1)  # Aplicar softmax para obtener probabilidades\n",
    "\n",
    "        # Calcular la contribución ponderada de cada m experto\n",
    "        m_combined = sum([m_expert*routing_probs[0, idx] for idx, m_expert in enumerate(self.m_experts)])\n",
    "        new_weight = m_combined * directional_component\n",
    "        \n",
    "        return F.linear(x, new_weight, self.linear.bias)\n",
    "    \n",
    "# Lora neurons expert\n",
    "\n",
    "class LoRAMixtureOfExpertsLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha, num_experts):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        # Experts B\n",
    "        self.B = nn.ParameterList([nn.Parameter(torch.zeros(rank, out_dim)) for _ in range(num_experts)])\n",
    "        self.alpha = alpha\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the linear transformation to the opertation x*A\n",
    "        weights = F.softmax(self.alpha * (x @ self.A), dim=-1)\n",
    "        \n",
    "        # Now, weights can be used to weigh the contribution of each expert more differentially. \n",
    "        # For simplicity, here we will sum the outputs as before. \n",
    "        # TODO: Consider modifying this to use the weights effectively.\n",
    "        expert_outputs = [weights @ b for b in self.B]\n",
    "        x = sum(expert_outputs) / self.num_experts\n",
    "        return x\n",
    "    \n",
    "class LinearWithLoRAMixtureOfExperts(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha, num_experts):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRAMixtureOfExpertsLayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha, num_experts\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.linear.weight, self.linear.bias) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            # Recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "source": [
    "model_lora = copy.deepcopy(model)\n",
    "\n",
    "model_lora.layers[0] = LinearWithLoRAMerged(model.layers[0], rank=4, alpha=8) # alpha = 2*rank\n",
    "model_lora.layers[2] = LinearWithLoRAMerged(model.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4] = LinearWithLoRAMerged(model.layers[4], rank=4, alpha=8)\n",
    "\n",
    "\n",
    "freeze_linear_layers(model_lora)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#wandb_logger_lora = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"lora\", group=\"lora\", save_dir=\"lightning_logs\")\n",
    "trainer_lora = Trainer(max_epochs=num_epochs) #, logger=wandb_logger_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 142 K \n",
      "--------------------------------------\n",
      "6.2 K     Trainable params\n",
      "136 K     Non-trainable params\n",
      "142 K     Total params\n",
      "0.569     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:08<00:00, 105.78it/s, v_num=37, train_loss_step=0.0321, train_acc_step=1.000, val_loss_step=0.00929, val_acc_step=1.000, val_loss_epoch=0.0883, val_acc_epoch=0.973, train_loss_epoch=0.0903, train_acc_epoch=0.972]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:08<00:00, 105.55it/s, v_num=37, train_loss_step=0.0321, train_acc_step=1.000, val_loss_step=0.00929, val_acc_step=1.000, val_loss_epoch=0.0883, val_acc_epoch=0.973, train_loss_epoch=0.0903, train_acc_epoch=0.972]\n"
     ]
    }
   ],
   "source": [
    "trainer_lora.fit(model_lora, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 229.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9672999978065491     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11241147667169571    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9672999978065491    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11241147667169571   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_lora.test(model_lora, mnist_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.m: True\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.m: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.m: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "source": [
    "model_dora = copy.deepcopy(model)\n",
    "\n",
    "model_dora.layers[0] = LinearWithDoRAMerged(model.layers[0], rank=4, alpha=8)\n",
    "model_dora.layers[2] = LinearWithDoRAMerged(model.layers[2], rank=4, alpha=8)\n",
    "model_dora.layers[4] = LinearWithDoRAMerged(model.layers[4], rank=4, alpha=8)\n",
    "\n",
    "freeze_linear_layers(model_dora)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_dora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#wandb_logger_dora = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"dora\", group=\"dora\", save_dir=\"lightning_logs\")\n",
    "trainer_dora = Trainer(max_epochs=num_epochs) #, logger=wandb_logger_dora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 143 K \n",
      "--------------------------------------\n",
      "7.4 K     Trainable params\n",
      "136 K     Non-trainable params\n",
      "143 K     Total params\n",
      "0.574     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:09<00:00, 92.27it/s, v_num=38, train_loss_step=0.0217, train_acc_step=1.000, val_loss_step=0.214, val_acc_step=0.875, val_loss_epoch=0.0727, val_acc_epoch=0.976, train_loss_epoch=0.0746, train_acc_epoch=0.977]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:09<00:00, 92.07it/s, v_num=38, train_loss_step=0.0217, train_acc_step=1.000, val_loss_step=0.214, val_acc_step=0.875, val_loss_epoch=0.0727, val_acc_epoch=0.976, train_loss_epoch=0.0746, train_acc_epoch=0.977]\n"
     ]
    }
   ],
   "source": [
    "trainer_dora.fit(model_dora, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 225.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9682000279426575     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10614069551229477    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9682000279426575    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10614069551229477   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.10614069551229477, 'test_acc': 0.9682000279426575}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_dora.test(model_dora, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lora Moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora_moe = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora_moe.layers[0] = LinearWithLoRAMixtureOfExperts(model_lora_moe.layers[0], rank=4, alpha=8, num_experts=8)\n",
    "model_lora_moe.layers[2] = LinearWithLoRAMixtureOfExperts(model_lora_moe.layers[2], rank=4, alpha=8, num_experts=8)\n",
    "model_lora_moe.layers[4] = LinearWithLoRAMixtureOfExperts(model_lora_moe.layers[4], rank=4, alpha=8, num_experts=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B.0: True\n",
      "layers.0.lora.B.1: True\n",
      "layers.0.lora.B.2: True\n",
      "layers.0.lora.B.3: True\n",
      "layers.0.lora.B.4: True\n",
      "layers.0.lora.B.5: True\n",
      "layers.0.lora.B.6: True\n",
      "layers.0.lora.B.7: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B.0: True\n",
      "layers.2.lora.B.1: True\n",
      "layers.2.lora.B.2: True\n",
      "layers.2.lora.B.3: True\n",
      "layers.2.lora.B.4: True\n",
      "layers.2.lora.B.5: True\n",
      "layers.2.lora.B.6: True\n",
      "layers.2.lora.B.7: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B.0: True\n",
      "layers.4.lora.B.1: True\n",
      "layers.4.lora.B.2: True\n",
      "layers.4.lora.B.3: True\n",
      "layers.4.lora.B.4: True\n",
      "layers.4.lora.B.5: True\n",
      "layers.4.lora.B.6: True\n",
      "layers.4.lora.B.7: True\n"
     ]
    }
   ],
   "source": [
    "freeze_linear_layers(model_lora_moe)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_lora_moe.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#wandb_logger_lora_moe = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"lora_moe\", group=\"lora_moe\", save_dir=\"lightning_logs\")\n",
    "trainer_lora_moe = Trainer(max_epochs=num_epochs) #, logger=wandb_logger_lora_moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 153 K \n",
      "--------------------------------------\n",
      "17.3 K    Trainable params\n",
      "136 K     Non-trainable params\n",
      "153 K     Total params\n",
      "0.613     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:11<00:00, 77.27it/s, v_num=39, train_loss_step=0.0452, train_acc_step=1.000, val_loss_step=0.00486, val_acc_step=1.000, val_loss_epoch=0.0552, val_acc_epoch=0.981, train_loss_epoch=0.0559, train_acc_epoch=0.983]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:11<00:00, 77.13it/s, v_num=39, train_loss_step=0.0452, train_acc_step=1.000, val_loss_step=0.00486, val_acc_step=1.000, val_loss_epoch=0.0552, val_acc_epoch=0.981, train_loss_epoch=0.0559, train_acc_epoch=0.983]\n"
     ]
    }
   ],
   "source": [
    "trainer_lora_moe.fit(model_lora_moe, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 200.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.975600004196167     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.08494822680950165    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.975600004196167    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08494822680950165   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_lora_moe.test(model_lora_moe, mnist_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dora Moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.routing_weights: True\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.0.m_experts.0: True\n",
      "layers.0.m_experts.1: True\n",
      "layers.0.m_experts.2: True\n",
      "layers.0.m_experts.3: True\n",
      "layers.0.m_experts.4: True\n",
      "layers.0.m_experts.5: True\n",
      "layers.0.m_experts.6: True\n",
      "layers.0.m_experts.7: True\n",
      "layers.0.m_experts.8: True\n",
      "layers.0.m_experts.9: True\n",
      "layers.0.m_experts.10: True\n",
      "layers.0.m_experts.11: True\n",
      "layers.0.m_experts.12: True\n",
      "layers.0.m_experts.13: True\n",
      "layers.0.m_experts.14: True\n",
      "layers.0.m_experts.15: True\n",
      "layers.2.routing_weights: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.2.m_experts.0: True\n",
      "layers.2.m_experts.1: True\n",
      "layers.2.m_experts.2: True\n",
      "layers.2.m_experts.3: True\n",
      "layers.2.m_experts.4: True\n",
      "layers.2.m_experts.5: True\n",
      "layers.2.m_experts.6: True\n",
      "layers.2.m_experts.7: True\n",
      "layers.4.routing_weights: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n",
      "layers.4.m_experts.0: True\n",
      "layers.4.m_experts.1: True\n",
      "layers.4.m_experts.2: True\n",
      "layers.4.m_experts.3: True\n"
     ]
    }
   ],
   "source": [
    "model_dora_moe = copy.deepcopy(model)\n",
    "model_dora_moe.layers[0] = LinearWithDoRAMoE(model_dora_moe.layers[0], rank=4, alpha=8, num_experts=16)\n",
    "model_dora_moe.layers[2] = LinearWithDoRAMoE(model_dora_moe.layers[2], rank=4, alpha=8, num_experts=8)\n",
    "model_dora_moe.layers[4] = LinearWithDoRAMoE(model_dora_moe.layers[4], rank=4, alpha=8, num_experts=4)\n",
    "freeze_linear_layers(model_dora_moe)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_dora_moe.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#wandb_logger_lora_moe = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"Dora_M_moe\", group=\"Dora_M_moe\", save_dir=\"lightning_logs\")\n",
    "trainer_dora_moe = Trainer(max_epochs=num_epochs) #, logger=wandb_logger_lora_moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 146 K \n",
      "--------------------------------------\n",
      "10.4 K    Trainable params\n",
      "136 K     Non-trainable params\n",
      "146 K     Total params\n",
      "0.586     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:13<00:00, 62.94it/s, v_num=40, train_loss_step=0.183, train_acc_step=0.958, val_loss_step=0.000567, val_acc_step=1.000, val_loss_epoch=0.0857, val_acc_epoch=0.975, train_loss_epoch=0.0864, train_acc_epoch=0.974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:13<00:00, 62.89it/s, v_num=40, train_loss_step=0.183, train_acc_step=0.958, val_loss_step=0.000567, val_acc_step=1.000, val_loss_epoch=0.0857, val_acc_epoch=0.975, train_loss_epoch=0.0864, train_acc_epoch=0.974]\n"
     ]
    }
   ],
   "source": [
    "trainer_dora_moe.fit(model_dora_moe, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 159.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9682000279426575     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10055254399776459    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9682000279426575    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10055254399776459   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.10055254399776459, 'test_acc': 0.9682000279426575}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_dora_moe.test(model_dora_moe, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁█</td></tr><tr><td>train_acc_step</td><td>▆▆▅▆▆▆▅▃▁▆▆▁█▅▃▃▆▅▆▆█▃█▆▅▆▆▅▃▆▃█▃▃</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>▃▃▄▃▂▂▄▆▅▂▂▄▁▄▅▅▃▆▃▅▁█▂▃▃▂▂▃▅▄█▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂█</td></tr><tr><td>val_acc_epoch</td><td>▁█</td></tr><tr><td>val_acc_step</td><td>▇▅▅▆█▆▇▇▃▁▆▅▇█▇▇▆▅▆▆▃▆▆▇▇▇▆▅▆▇▆▅▅▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▂▄▆▄▂▄▃▂▄▇▂▅▄▂▂▃▂▃█▃▅▂▄▄▂▂▃▄▃▂▂▅▇▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_acc</td><td>0.9667</td></tr><tr><td>test_loss</td><td>0.11059</td></tr><tr><td>train_acc_epoch</td><td>0.9754</td></tr><tr><td>train_acc_step</td><td>0.95312</td></tr><tr><td>train_loss_epoch</td><td>0.08196</td></tr><tr><td>train_loss_step</td><td>0.07976</td></tr><tr><td>trainer/global_step</td><td>1720</td></tr><tr><td>val_acc_epoch</td><td>0.976</td></tr><tr><td>val_acc_step</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.0828</td></tr><tr><td>val_loss_step</td><td>0.0001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Dora_M_moe</strong> at: <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/copaly78' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/copaly78</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>lightning_logs\\wandb\\run-20240329_214136-copaly78\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
