{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "    \n",
    "from typing import List\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 2024\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project_name = 'MNIST_LORA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download only\n",
    "        datasets.MNIST(root=self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(root=self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Transform and split datasets\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = datasets.MNIST(root=self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = datasets.MNIST(root=self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "# Example of how to use the MNISTDataModule\n",
    "batch_size = 64\n",
    "mnist_data = MNISTDataModule(data_dir='./data', batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "random_seed = 123\n",
    "learning_rate = 0.005\n",
    "num_epochs = 2\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 256\n",
    "num_classes = 10\n",
    "\n",
    "class MultilayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('val_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log('test_loss', loss)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "model = MultilayerPerceptron(num_features, num_hidden_1, num_hidden_2, num_classes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"baseline\", group=\"baseline\", save_dir=\"lightning_logs\")\n",
    "trainer = Trainer(max_epochs=num_epochs, logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiguel_kjh\u001b[0m (\u001b[33msiani-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>lightning_logs\\wandb\\run-20240321_183349-ha0f8pbq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/ha0f8pbq' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/ha0f8pbq' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/ha0f8pbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 136 K \n",
      "--------------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "0.544     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:09<00:00, 92.29it/s, v_num=8pbq, train_loss_step=0.0947, train_acc_step=0.917, val_loss_step=0.000484, val_acc_step=1.000, val_loss_epoch=0.128, val_acc_epoch=0.964, train_loss_epoch=0.115, train_acc_epoch=0.965]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:09<00:00, 88.44it/s, v_num=8pbq, train_loss_step=0.0947, train_acc_step=0.917, val_loss_step=0.000484, val_acc_step=1.000, val_loss_epoch=0.128, val_acc_epoch=0.964, train_loss_epoch=0.115, train_acc_epoch=0.965]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 279.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9661999940872192     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11397630721330643    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9661999940872192    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11397630721330643   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.11397630721330643, 'test_acc': 0.9661999940872192}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁█</td></tr><tr><td>train_acc_step</td><td>▁▃▃▆▆▇▅▄▁▅▇▅▄█▅▄▇▇▅▇▆▆▅█▂▆▇▆▅█▆███</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>▆▅▄▃▂▃▄▃█▃▁▃▄▁▃▄▂▂▅▁▂▂▃▁▄▂▁▂▃▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂█</td></tr><tr><td>val_acc_epoch</td><td>▁█</td></tr><tr><td>val_acc_step</td><td>▃▅▅▆▄▅▄▇▇▃▁▅▄▅▇▄▅▅▂▆█▆▅▇▅█▅▆▆▅▆▆▅▅▅▄█▆▇█</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▆▅▅▃▅▃▆▂▄██▄▇▆▃▄▅▃█▄▂▃▅▃▅▁▇▅▃▇▃▄▅▄▄▄▁▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_acc</td><td>0.9662</td></tr><tr><td>test_loss</td><td>0.11398</td></tr><tr><td>train_acc_epoch</td><td>0.96456</td></tr><tr><td>train_acc_step</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.11487</td></tr><tr><td>train_loss_step</td><td>0.02563</td></tr><tr><td>trainer/global_step</td><td>1720</td></tr><tr><td>val_acc_epoch</td><td>0.9638</td></tr><tr><td>val_acc_step</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.12841</td></tr><tr><td>val_loss_step</td><td>0.00048</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline</strong> at: <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/ha0f8pbq' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/ha0f8pbq</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>lightning_logs\\wandb\\run-20240321_183349-ha0f8pbq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# This LoRA code is equivalent to LinearWithLoRA\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)\n",
    "\n",
    "    \n",
    "# This DoRA code is equivalent to LinearWithDoRA\n",
    "# Code inspired by https://github.com/catid/dora/blob/main/dora.py\n",
    "class LinearWithDoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "        \n",
    "        self.m = nn.Parameter(\n",
    "            self.linear.weight.norm(p=2, dim=0, keepdim=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        numerator = self.linear.weight + self.lora.alpha*lora.T\n",
    "        denominator = numerator.norm(p=2, dim=0, keepdim=True)\n",
    "        directional_component = numerator / denominator\n",
    "        new_weight = self.m * directional_component\n",
    "        return F.linear(x, new_weight, self.linear.bias)\n",
    "    \n",
    "# Lora neurons expert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRAMixtureOfExpertsLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha, num_experts):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        # Lista de expertos B\n",
    "        self.B = nn.ParameterList([nn.Parameter(torch.zeros(rank, out_dim)) for _ in range(num_experts)])\n",
    "        self.alpha = alpha\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplicar softmax después de x * A\n",
    "        weights = F.softmax(self.alpha * (x @ self.A), dim=-1)\n",
    "        \n",
    "        # Ahora, weights puede ser utilizado para ponderar la contribución de cada experto de manera más diferenciada.\n",
    "        # Por simplicidad, aquí sumaremos las salidas como antes. Considera modificar esto para utilizar los pesos de manera efectiva.\n",
    "        expert_outputs = [weights @ b for b in self.B]\n",
    "        x = sum(expert_outputs) / self.num_experts\n",
    "        return x\n",
    "    \n",
    "class LinearWithLoRAMixtureOfExperts(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha, num_experts):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRAMixtureOfExpertsLayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha, num_experts\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.linear.weight, self.linear.bias) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model_lora = copy.deepcopy(model)\n",
    "model_dora = copy.deepcopy(model)\n",
    "model_lora_moe = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptron(\n",
       "  (layers): Sequential(\n",
       "    (0): LinearWithLoRAMerged(\n",
       "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): LinearWithLoRAMerged(\n",
       "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): LinearWithLoRAMerged(\n",
       "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora.layers[0] = LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
    "model_lora.layers[2] = LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4] = LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
    "model_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptron(\n",
       "  (layers): Sequential(\n",
       "    (0): LinearWithDoRAMerged(\n",
       "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): LinearWithDoRAMerged(\n",
       "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): LinearWithDoRAMerged(\n",
       "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.layers[0] = LinearWithDoRAMerged(model_dora.layers[0], rank=4, alpha=8)\n",
    "model_dora.layers[2] = LinearWithDoRAMerged(model_dora.layers[2], rank=4, alpha=8)\n",
    "model_dora.layers[4] = LinearWithDoRAMerged(model_dora.layers[4], rank=4, alpha=8)\n",
    "model_dora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            # Recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "source": [
    "freeze_linear_layers(model_lora)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger_lora = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"lora\", group=\"lora\", save_dir=\"lightning_logs\")\n",
    "trainer_lora = Trainer(max_epochs=num_epochs, logger=wandb_logger_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>lightning_logs\\wandb\\run-20240321_183414-1451qfz3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/1451qfz3' target=\"_blank\">lora</a></strong> to <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/1451qfz3' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/1451qfz3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 142 K \n",
      "--------------------------------------\n",
      "6.2 K     Trainable params\n",
      "136 K     Non-trainable params\n",
      "142 K     Total params\n",
      "0.569     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:09<00:00, 88.78it/s, v_num=qfz3, train_loss_step=0.377, train_acc_step=0.958, val_loss_step=0.00304, val_acc_step=1.000, val_loss_epoch=0.0855, val_acc_epoch=0.975, train_loss_epoch=0.090, train_acc_epoch=0.973]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:10<00:00, 84.96it/s, v_num=qfz3, train_loss_step=0.377, train_acc_step=0.958, val_loss_step=0.00304, val_acc_step=1.000, val_loss_epoch=0.0855, val_acc_epoch=0.975, train_loss_epoch=0.090, train_acc_epoch=0.973]\n"
     ]
    }
   ],
   "source": [
    "trainer_lora.fit(model_lora, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 247.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9679999947547913     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10649599879980087    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9679999947547913    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10649599879980087   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁█</td></tr><tr><td>train_acc_step</td><td>▇▅▃▅▁▇▅▃▂██▇█▆█▇▂█▆▆▅▅▇▇▆▇▇▆▃▇▅▆▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>▇▄█▇█▃▃▄▇▁▁▄▁▃▁▂█▁▄▄▅▇▂▂▆▂▄▃▇▂▃▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂█</td></tr><tr><td>val_acc_epoch</td><td>▁█</td></tr><tr><td>val_acc_step</td><td>▇▆▅▆█▆▆▆▆▅▆▁▆██▇▆▅▇▇▆▆▅▅▅▆▇▇▆▇█▇▇▆▆▆▇▅▇█</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▂▃▅▃▁▅▃▅▃▃▂▅▃▁▁▂▂▄▄▂▃█▄▃▃▃▂▂▃▂▁▂▂▃▃▂▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_acc</td><td>0.968</td></tr><tr><td>test_loss</td><td>0.1065</td></tr><tr><td>train_acc_epoch</td><td>0.97251</td></tr><tr><td>train_acc_step</td><td>0.96875</td></tr><tr><td>train_loss_epoch</td><td>0.09001</td></tr><tr><td>train_loss_step</td><td>0.06095</td></tr><tr><td>trainer/global_step</td><td>1720</td></tr><tr><td>val_acc_epoch</td><td>0.9748</td></tr><tr><td>val_acc_step</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.08546</td></tr><tr><td>val_loss_step</td><td>0.00304</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora</strong> at: <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/1451qfz3' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/1451qfz3</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>lightning_logs\\wandb\\run-20240321_183414-1451qfz3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_lora.test(model_lora, mnist_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.m: True\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.m: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.m: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "source": [
    "freeze_linear_layers(model_dora)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_dora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger_dora = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"dora\", group=\"dora\", save_dir=\"lightning_logs\")\n",
    "trainer_dora = Trainer(max_epochs=num_epochs, logger=wandb_logger_dora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>lightning_logs\\wandb\\run-20240321_183443-m65mctjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/m65mctjo' target=\"_blank\">dora</a></strong> to <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/m65mctjo' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/m65mctjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 143 K \n",
      "--------------------------------------\n",
      "7.4 K     Trainable params\n",
      "136 K     Non-trainable params\n",
      "143 K     Total params\n",
      "0.574     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:10<00:00, 78.72it/s, v_num=ctjo, train_loss_step=0.0252, train_acc_step=1.000, val_loss_step=0.751, val_acc_step=0.875, val_loss_epoch=0.0725, val_acc_epoch=0.979, train_loss_epoch=0.0767, train_acc_epoch=0.976] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:11<00:00, 75.73it/s, v_num=ctjo, train_loss_step=0.0252, train_acc_step=1.000, val_loss_step=0.751, val_acc_step=0.875, val_loss_epoch=0.0725, val_acc_epoch=0.979, train_loss_epoch=0.0767, train_acc_epoch=0.976]\n"
     ]
    }
   ],
   "source": [
    "trainer_dora.fit(model_dora, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 229.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9674000144004822     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10446048527956009    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9674000144004822    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10446048527956009   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁█</td></tr><tr><td>train_acc_step</td><td>▁▅▆▅▆▅█▁█▅▃▅▅▆▆▃▅▆▆▃█▃▆▃█▃▅█▃▅▆▃▅▅</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>▆▄▂▃▂▃▂▆▂▃▄▅▃▂▂▄▆▂▂█▂▆▄▅▁▆▃▁▆▃▃▄▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂█</td></tr><tr><td>val_acc_epoch</td><td>▁█</td></tr><tr><td>val_acc_step</td><td>▅▇▅▆██▇▇▇▆▇▅███▅█▇▆▅▇▅██▇▇▅█▆▆▆██▇▄█▆▇▇▁</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▂▂▃▂▁▁▂▂▁▂▂▃▁▁▁▃▁▁▂▂▂▂▁▁▁▁▂▁▁▂▂▁▁▂▄▁▂▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_acc</td><td>0.9674</td></tr><tr><td>test_loss</td><td>0.10446</td></tr><tr><td>train_acc_epoch</td><td>0.97631</td></tr><tr><td>train_acc_step</td><td>0.96875</td></tr><tr><td>train_loss_epoch</td><td>0.07667</td></tr><tr><td>train_loss_step</td><td>0.14442</td></tr><tr><td>trainer/global_step</td><td>1720</td></tr><tr><td>val_acc_epoch</td><td>0.9788</td></tr><tr><td>val_acc_step</td><td>0.875</td></tr><tr><td>val_loss_epoch</td><td>0.07249</td></tr><tr><td>val_loss_step</td><td>0.75073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dora</strong> at: <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/m65mctjo' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/m65mctjo</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>lightning_logs\\wandb\\run-20240321_183443-m65mctjo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_dora.test(model_dora, mnist_data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Lora Moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class RegularizedMLP(MultilayerPerceptron):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes, learning_rate, regularization_type='cosine', lambda_reg=0.01):\n",
    "        super().__init__(num_features, num_hidden_1, num_hidden_2, num_classes, learning_rate)\n",
    "        \n",
    "        self.regularization_type = regularization_type\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def apply_regularization(self, outputs):\n",
    "        if self.regularization_type == 'cosine':\n",
    "            # Calcula la pérdida de regularización basada en la distancia coseno entre pares de salidas\n",
    "            cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            regularization_loss = sum(cos(outputs[i], outputs[j]) for i, j in combinations(range(len(outputs)), 2)) / combinations(len(outputs), 2)\n",
    "        elif self.regularization_type == 'kl':\n",
    "            # Calcula la pérdida de regularización basada en la divergencia KL entre pares de salidas\n",
    "            kl_div = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "            regularization_loss = sum(kl_div(F.log_softmax(outputs[i], dim=1), F.softmax(outputs[j], dim=1)) for i, j in combinations(range(len(outputs)), 2)) / combinations(len(outputs), 2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported regularization type\")\n",
    "        return regularization_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        features, targets = batch\n",
    "        features = features.view(-1, 28*28)\n",
    "        logits = self(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        # Aplicar regularización si lambda_reg > 0\n",
    "        if self.lambda_reg > 0:\n",
    "            # Asumiendo que quieres regularizar basado en las salidas de cada capa lineal\n",
    "            # Necesitarás ajustar este paso para extraer las salidas intermedias si es necesario\n",
    "            intermediate_outputs = [layer(features) for layer in self.layers if isinstance(layer, nn.Linear)]\n",
    "            regularization_loss = self.apply_regularization(intermediate_outputs)\n",
    "            loss += self.lambda_reg * regularization_loss\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == targets).float().mean()\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora_moe = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora_moe.layers[0] = LinearWithLoRAMixtureOfExperts(model_lora_moe.layers[0], rank=4, alpha=8, num_experts=8)\n",
    "model_lora_moe.layers[2] = LinearWithLoRAMixtureOfExperts(model_lora_moe.layers[2], rank=4, alpha=8, num_experts=8)\n",
    "model_lora_moe.layers[4] = LinearWithLoRAMixtureOfExperts(model_lora_moe.layers[4], rank=4, alpha=8, num_experts=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRAMixtureOfExperts(\n",
      "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "      (lora): LoRAMixtureOfExpertsLayer(\n",
      "        (B): ParameterList(\n",
      "            (0): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (1): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (2): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (3): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (4): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (5): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (6): Parameter containing: [torch.float32 of size 4x128]\n",
      "            (7): Parameter containing: [torch.float32 of size 4x128]\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRAMixtureOfExperts(\n",
      "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (lora): LoRAMixtureOfExpertsLayer(\n",
      "        (B): ParameterList(\n",
      "            (0): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (1): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (2): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (3): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (4): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (5): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (6): Parameter containing: [torch.float32 of size 4x256]\n",
      "            (7): Parameter containing: [torch.float32 of size 4x256]\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRAMixtureOfExperts(\n",
      "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
      "      (lora): LoRAMixtureOfExpertsLayer(\n",
      "        (B): ParameterList(\n",
      "            (0): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (1): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (2): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (3): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (4): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (5): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (6): Parameter containing: [torch.float32 of size 4x10]\n",
      "            (7): Parameter containing: [torch.float32 of size 4x10]\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lora_moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B.0: True\n",
      "layers.0.lora.B.1: True\n",
      "layers.0.lora.B.2: True\n",
      "layers.0.lora.B.3: True\n",
      "layers.0.lora.B.4: True\n",
      "layers.0.lora.B.5: True\n",
      "layers.0.lora.B.6: True\n",
      "layers.0.lora.B.7: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B.0: True\n",
      "layers.2.lora.B.1: True\n",
      "layers.2.lora.B.2: True\n",
      "layers.2.lora.B.3: True\n",
      "layers.2.lora.B.4: True\n",
      "layers.2.lora.B.5: True\n",
      "layers.2.lora.B.6: True\n",
      "layers.2.lora.B.7: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B.0: True\n",
      "layers.4.lora.B.1: True\n",
      "layers.4.lora.B.2: True\n",
      "layers.4.lora.B.3: True\n",
      "layers.4.lora.B.4: True\n",
      "layers.4.lora.B.5: True\n",
      "layers.4.lora.B.6: True\n",
      "layers.4.lora.B.7: True\n"
     ]
    }
   ],
   "source": [
    "freeze_linear_layers(model_lora_moe)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_lora_moe.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger_lora_moe = WandbLogger(project=wandb_project_name, log_model=\"all\", name=\"lora_moe\", group=\"lora_moe\", save_dir=\"lightning_logs\")\n",
    "trainer_lora_moe = Trainer(max_epochs=num_epochs, logger=wandb_logger_lora_moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>lightning_logs\\wandb\\run-20240321_183514-xp8g21ia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/xp8g21ia' target=\"_blank\">lora_moe</a></strong> to <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/siani-ai/MNIST_LORA' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/xp8g21ia' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/xp8g21ia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 153 K \n",
      "--------------------------------------\n",
      "17.3 K    Trainable params\n",
      "136 K     Non-trainable params\n",
      "153 K     Total params\n",
      "0.613     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:14<00:00, 58.68it/s, v_num=21ia, train_loss_step=0.0452, train_acc_step=1.000, val_loss_step=0.00486, val_acc_step=1.000, val_loss_epoch=0.0552, val_acc_epoch=0.981, train_loss_epoch=0.0559, train_acc_epoch=0.983]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 860/860 [00:15<00:00, 56.50it/s, v_num=21ia, train_loss_step=0.0452, train_acc_step=1.000, val_loss_step=0.00486, val_acc_step=1.000, val_loss_epoch=0.0552, val_acc_epoch=0.981, train_loss_epoch=0.0559, train_acc_epoch=0.983]\n"
     ]
    }
   ],
   "source": [
    "trainer_lora_moe.fit(model_lora_moe, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\43294881\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 206.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.975600004196167     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.08494822680950165    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.975600004196167    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08494822680950165   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁█</td></tr><tr><td>train_acc_step</td><td>███▁▆█▃▁▆█▃▃▆▆██▁▆█▆▆▁▆▆██▆█▃▃█▃█▆</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>▂▁▂▇▃▁▃▇▄▁▆▆▃▃▂▂▅▃▂▃▂▅▄▄▁▁▃▃█▃▁▅▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂█</td></tr><tr><td>val_acc_epoch</td><td>▁█</td></tr><tr><td>val_acc_step</td><td>▂▁▅█▅█▅▅▅▂▇▇▅▄█▇▇▇▇▇▇█▇▄█▅█▄▇▅▇▇█▇█▅▇▇██</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▆▆▇▁▃▂▄▄▄█▇▂▄▄▂▄▃▂▃▂▃▁▂▄▂▅▁▅▃▄▃▃▁▂▁▅▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_acc</td><td>0.9756</td></tr><tr><td>test_loss</td><td>0.08495</td></tr><tr><td>train_acc_epoch</td><td>0.98311</td></tr><tr><td>train_acc_step</td><td>0.98438</td></tr><tr><td>train_loss_epoch</td><td>0.05593</td></tr><tr><td>train_loss_step</td><td>0.04594</td></tr><tr><td>trainer/global_step</td><td>1720</td></tr><tr><td>val_acc_epoch</td><td>0.9814</td></tr><tr><td>val_acc_step</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.05517</td></tr><tr><td>val_loss_step</td><td>0.00486</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora_moe</strong> at: <a href='https://wandb.ai/siani-ai/MNIST_LORA/runs/xp8g21ia' target=\"_blank\">https://wandb.ai/siani-ai/MNIST_LORA/runs/xp8g21ia</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>lightning_logs\\wandb\\run-20240321_183514-xp8g21ia\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_lora_moe.test(model_lora_moe, mnist_data)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
