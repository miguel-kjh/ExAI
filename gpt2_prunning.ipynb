{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\ExAI\\exai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2Config\n",
    "from LLMHeadModelWithFFNOutput import LLMHeadModelWithFFNOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraciÃ³n del modelo y el tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "num_layers = config.n_layer\n",
    "num_neurons = config.hidden_size * 4\n",
    "max_length = 19\n",
    "index_number = -3\n",
    "test_file = os.path.join('datasets', 'test_dataset.txt')\n",
    "folder = os.path.join('experiments', \"activations\")\n",
    "file = os.path.join(folder, \"gpt2.pickle\")\n",
    "model = LLMHeadModelWithFFNOutput.from_pretrained('gpt2-finetuned', config=config)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_file, encoding=\"utf-8\") as file:\n",
    "    text = file.readlines()\n",
    "\n",
    "prompts = [t[:index_number].strip() for t in text]\n",
    "numbers = [t[index_number:].strip() for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, prompts: list, numbers: list) -> float:\n",
    "    accuracy = 0\n",
    "    activations = []\n",
    "    for prompt, number in tqdm(zip(prompts, numbers), desc=\"Testing...\"):\n",
    "        input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "        outputs_tokens = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "        output_text = tokenizer.decode(outputs_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        output_text = output_text[index_number:].strip()\n",
    "\n",
    "        activation_ffn = model.get_activation_ffn()\n",
    "\n",
    "        if output_text == number:\n",
    "            activations.append(activation_ffn)\n",
    "            accuracy += 1\n",
    "\n",
    "    return accuracy * 100 / len(prompts), activations\n",
    "\n",
    "def get_activation_ffn(model, prompts: list) -> list:\n",
    "    activations = []\n",
    "    for prompt in tqdm(prompts, desc=\"Getting activations...\"):\n",
    "        input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "        _ = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "        activation_ffn = model.get_activation_ffn()\n",
    "        activations.append(activation_ffn)\n",
    "\n",
    "    activation_ffn_array = activations\n",
    "\n",
    "    return activation_ffn_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing...: 1000it [00:30, 32.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc, activations = get_accuracy(model, prompts, numbers)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_ffn_array = np.stack(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 12, 1, 1, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(activation_ffn_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 297, 3072)\n"
     ]
    }
   ],
   "source": [
    "# reshape\n",
    "activation_ffn_array = activation_ffn_array.reshape(num_layers, -1, num_neurons)\n",
    "print(activation_ffn_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(\"gpt2.pickle\", 'wb') as f:\n",
    "    pickle.dump(activation_ffn_array, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_activations_neuron_per_layer(activations: np.array, layer: int, threshold: float = 0.3, n_components: int = 10) -> tuple:\n",
    "    first_layer_activations = activations[layer]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_activations = scaler.fit_transform(first_layer_activations.reshape(-1, first_layer_activations.shape[-1])).reshape(first_layer_activations.shape)\n",
    "\n",
    "    nmf = NMF(n_components=n_components)\n",
    "    w = nmf.fit_transform(normalized_activations.reshape(-1, normalized_activations.shape[-1]))  # Aplana las activaciones para NMF\n",
    "    h = nmf.components_\n",
    "    \n",
    "    low_contribution_neurons = np.where(h.mean(axis=0) < threshold)[0]\n",
    "\n",
    "    return low_contribution_neurons, w, h\n",
    "\n",
    "def plot_w_and_h_heatmaps(w: np.array, h: np.array, layer: int):\n",
    "    # Mapa de calor para W\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(w, cmap='viridis')\n",
    "    plt.title('W Heatmap - Layer {}'.format(layer))\n",
    "    plt.xlabel('Components')\n",
    "    plt.ylabel('Tokens/Samples')\n",
    "    plt.show()\n",
    "\n",
    "    # Mapa de calor para H\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(h, cmap='viridis')\n",
    "    plt.title('H Heatmap - Layer {}'.format(layer))\n",
    "    plt.xlabel('Neurons')\n",
    "    plt.ylabel('Components')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
